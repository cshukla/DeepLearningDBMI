{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.datasets import mnist\n",
    "\n",
    "##########################################################################\n",
    "# vae\n",
    "# \n",
    "# This function is heavily derived from - \n",
    "# \n",
    "# Inputs:\n",
    "#   - Y: matrix with gene names as counts and individual cells are rows\n",
    "#   - loss_fun: loss function for autoencoder\n",
    "#   - latent_dim: number of latent dimensions to project data into\n",
    "#   - intermediate_dim: number of dimensions in the hidden layer\n",
    "#   - batch_size: batch size used for training autoencoder\n",
    "#   - epochs: epochs for training autoencoder\n",
    "#   - nbshape: shape parameter for negative binomial loss. Defaults to 1\n",
    "# \n",
    "# Outputs:\n",
    "#   - x_encoded: Encoded transformation of input\n",
    "#   - z_mean: parameter for the mean in the latent space\n",
    "#\n",
    "##########################################################################\n",
    "\n",
    "def vae(Y, loss_fun, latent_dim, intermediate_dim, \n",
    "        batch_size, epochs, epsilon_std, nbshape=1):\n",
    "            \n",
    "    x_train = preprocess(Y, loss_fun)\n",
    "    original_dim = x_train.shape[1]\n",
    "    \n",
    "    # encoder network: map inputs to latent distribution parameters\n",
    "    x = Input(shape=(original_dim,))\n",
    "    h = Dense(intermediate_dim, activation='relu')(x)\n",
    "    z_mean = Dense(latent_dim)(h)\n",
    "    z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "    \n",
    "    # use 'sampling' function to sample new similar points from the latent space\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    \n",
    "    # map these sampled latent points back to reconstructed inputs \n",
    "    # i.e. a sort of decoder network\n",
    "    decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "    decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "    h_decoded = decoder_h(z)\n",
    "    x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "    # build end-to-end autoencoder\n",
    "    vae_model = Model(x, x_decoded_mean)\n",
    "    \n",
    "\n",
    "    # build encoder, from inputs to latent space\n",
    "    encoder = Model(x, z_mean)\n",
    "    \n",
    "\n",
    "    # Train the model using the end-to-end model, with a custom loss function: \n",
    "    # the sum of a reconstruction term, and the KL divergence regularization term.\n",
    "    # Use different user defined loss functions to generate reconstruction loss\n",
    "    vae_loss = vae_loss(loss_fun, x, x_decoded_mean, nbshape, z_log_sigma, z_mean)\n",
    "    vae_model.compile(optimizer='rmsprop', loss=vae_loss)\n",
    "    vae_model.summary()\n",
    "    \n",
    "    # fit the model with the given data\n",
    "    vae_model.fit(x_train, x_train, \n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2)\n",
    "\n",
    "    # build a model to project inputs on the latent space\n",
    "    encoder = Model(x_train, z_mean)\n",
    "    x_encoded = encoder.predict(x_train, batch_size=batch_size)\n",
    "    \n",
    "    return x_encoded, z_mean\n",
    "\n",
    "##########################################################################\n",
    "# preprocess\n",
    "#\n",
    "# Inputs:\n",
    "#   - Y: matrix with gene names as counts and individual cells are rows\n",
    "#   - loss_fun: loss function for autoencoder\n",
    "#\n",
    "# Outputs:\n",
    "#   - x_train: preprocessed matrix of counts for training the autoencoder\n",
    "##########################################################################\n",
    "\n",
    "def preprocess(Y, loss_fun):\n",
    "    if \n",
    "    \n",
    "    return x_train\n",
    "\n",
    "##########################################################################\n",
    "# sampling\n",
    "#\n",
    "# Inputs:\n",
    "#   -\n",
    "# \n",
    "# Outputs:\n",
    "#   -\n",
    "##########################################################################\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "##########################################################################\n",
    "# compute_model_loss\n",
    "# \n",
    "# Inputs:\n",
    "#   - loss_fun: loss function for autoencoder\n",
    "#   - x: preprocessed input to the autoencoder\n",
    "#   - x_decoded_mean: mean of the x decoded by the autoencoder\n",
    "#   - nbshape: shape parameter for negative binomial loss. Defaults to 1\n",
    "# \n",
    "# Outputs:\n",
    "#   - loss: loss from the autoencoding\n",
    "##########################################################################\n",
    "\n",
    "def vae_loss(loss_fun, x, x_decoded_mean, nbshape, z_log_sigma, z_mean):\n",
    "    if loss_fun == 'poisson':\n",
    "        reconstruction_loss = original_dim*poisson(x, x_decoded_mean)\n",
    "    elif loss_fun == 'negative_binomial':\n",
    "        reconstruction_loss = original_dim*negative_binomial(x, x_decoded_mean, nbshape)\n",
    "    elif loss_fun == 'gaussian':\n",
    "        reconstruction_loss = original_dim*gaussian(x, x_decoded_mean)\n",
    "    else:\n",
    "        reconstruction_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n",
    "    \n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "    \n",
    "    return reconstruction_loss + kl_loss\n",
    "\n",
    "# Question: Do we have training data here? Basically do we have a x_train and y_train?\n",
    "# For example, is it known what the gene expression of a particular cell type looks like?\n",
    "# Or is y always unknown?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
