{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.datasets import mnist\n",
    "\n",
    "############################################################################################\n",
    "# vae: build a variational autoencoder using the input data\n",
    "# \n",
    "# This function is heavily derived from - \n",
    "# \n",
    "# Inputs:\n",
    "#   - counts_mat: matrix with gene names as counts and individual cells are rows\n",
    "#   - loss_fun: loss function for autoencoder\n",
    "#   - latent_dim: number of latent dimensions to project data into\n",
    "#   - intermediate_dim: number of dimensions in the hidden layer\n",
    "#   - batch_size: batch size used for training autoencoder\n",
    "#   - epochs: epochs for training autoencoder\n",
    "#   - nbshape: shape parameter for negative binomial loss. Defaults to 1\n",
    "# \n",
    "# Outputs:\n",
    "#   - x_encoded: Encoded transformation of input\n",
    "#   - z_mean: parameter for the mean of distribution in the latent space\n",
    "#\n",
    "############################################################################################\n",
    "\n",
    "def vae(counts_mat, loss_fun, latent_dim, intermediate_dim, \n",
    "        batch_size, epochs, epsilon_std, nbshape=1):\n",
    "            \n",
    "    x_train = preprocess(counts_mat, loss_fun)\n",
    "    original_dim = x_train.shape[1]\n",
    "    \n",
    "    # encoder network: map inputs to latent distribution parameters\n",
    "    x = Input(shape=(original_dim,))\n",
    "    h = Dense(intermediate_dim, activation='relu')(x)\n",
    "    z_mean = Dense(latent_dim)(h)\n",
    "    z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "    \n",
    "    # use 'sampling' function to sample new similar points from the latent space\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    \n",
    "    # map these sampled latent points back to reconstructed inputs \n",
    "    # i.e. a sort of decoder network\n",
    "    decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "    decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "    h_decoded = decoder_h(z)\n",
    "    x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "    # build end-to-end autoencoder\n",
    "    vae_model = Model(x, x_decoded_mean)\n",
    "    \n",
    "\n",
    "    # build encoder, from inputs to latent space\n",
    "    encoder = Model(x, z_mean)\n",
    "    \n",
    "\n",
    "    # Train the model using the end-to-end model, with a custom loss function: \n",
    "    # the sum of a reconstruction term, and the KL divergence regularization term.\n",
    "    # Use different user defined loss functions to generate reconstruction loss\n",
    "    vae_loss = vae_loss(loss_fun, x, x_decoded_mean, nbshape, z_log_sigma, z_mean)\n",
    "    vae_model.compile(optimizer='rmsprop', loss=vae_loss)\n",
    "    vae_model.summary()\n",
    "    \n",
    "    \n",
    "    # fit the model with the given data\n",
    "    vae_model.fit(x_train, x_train, \n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2)\n",
    "    \n",
    "\n",
    "    # project/encode inputs on the latent space\n",
    "    encoder = Model(x_train, z_mean)\n",
    "    x_encoded = encoder.predict(x_train, batch_size=batch_size)\n",
    "    \n",
    "    \n",
    "    return x_encoded, z_mean\n",
    "\n",
    "############################################################################################\n",
    "# preprocess: preprocess counts matrix to make it suitable for particular loss function\n",
    "#\n",
    "# Inputs:\n",
    "#   - Y: matrix with gene names as counts and individual cells are rows\n",
    "#   - loss_fun: loss function for autoencoder\n",
    "#\n",
    "# Outputs:\n",
    "#   - x_train: preprocessed matrix of counts for training the autoencoder\n",
    "############################################################################################\n",
    "\n",
    "def preprocess(counts_mat, loss_fun):\n",
    "    if loss_fun == 'gaussian':\n",
    "        x_train = np.log2(1 + (counts_mat.T/np.sum(counts_mat, axis=1)).T * 1e+6) # log2(1+Y/rowSums(Y)*1e6)\n",
    "        x_train = scale(x_train, axis=1, with_mean=True, with_std=True, copy=True) # scale_center(Y)\n",
    "        \n",
    "    elif loss_fun == 'bernoulli':\n",
    "        x_train = counts_mat\n",
    "        x_train[x_train > 0] = 1 # int(Y>0)\n",
    "        \n",
    "    else:\n",
    "        x_train = counts_mat\n",
    "    \n",
    "    return x_train\n",
    "\n",
    "############################################################################################\n",
    "# sampling: sample encoded values of the input using latent space parameters\n",
    "#\n",
    "# Inputs:\n",
    "#   - args: mean and variance parameters of the latent space distribution\n",
    "# \n",
    "# Outputs:\n",
    "#   - encoded points sampled from the latent space distribution\n",
    "############################################################################################\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "############################################################################################\n",
    "# vae_loss: compute loss of variational encoder adding reconstruction & regularization loss\n",
    "# \n",
    "# Inputs:\n",
    "#   - loss_fun: loss function for autoencoder\n",
    "#   - x: preprocessed input to the autoencoder\n",
    "#   - x_decoded_mean: mean of the x decoded by the autoencoder\n",
    "#   - nbshape: shape parameter for negative binomial loss. Defaults to 1\n",
    "# \n",
    "# Outputs:\n",
    "#   - loss: loss from the autoencoding\n",
    "############################################################################################\n",
    "\n",
    "def vae_loss(loss_fun, x, x_decoded_mean, nbshape, z_log_sigma, z_mean):\n",
    "    if loss_fun == 'poisson':\n",
    "        reconstruction_loss = objectives.poisson(x, x_decoded_mean)\n",
    "        \n",
    "    elif loss_fun == 'negative_binomial':\n",
    "        reconstruction_loss = negative_binomial_loss(x, x_decoded_mean, nbshape)\n",
    "        \n",
    "    elif loss_fun == 'gaussian':\n",
    "        reconstruction_loss = gaussian_loss(x, x_decoded_mean)\n",
    "        \n",
    "    elif loss_fun == 'bernoulli':\n",
    "        reconstruction_loss = bernoulli_loss(x, x_decoded_mean)\n",
    "        \n",
    "    else:\n",
    "        reconstruction_loss = objectives.binary_crossentropy(x, x_decoded_mean) # defaults to cross entropy\n",
    "    \n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "    \n",
    "    return reconstruction_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 3, 4],\n",
       "       [0, 5, 1, 4]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These next cells show that preprocessing is working as expected\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# lets see what the input matrix looks like\n",
    "np.array([[0, 1, 3, 4], [0, 5, 1, 4]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.7242733 ,  0.42319561,  0.62421885,  0.67685884],\n",
       "       [-1.72094179,  0.68567751,  0.39051082,  0.64475346]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing for gaussian\n",
    "x = np.array([[0, 1, 3, 4], [0, 5, 1, 4]])\n",
    "y = np.log2(1 + (x.T/np.sum(x, axis=1)).T * 1e+6)\n",
    "y = scale(y, axis=1, with_mean=True, with_std=True, copy=True )\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1],\n",
       "       [0, 1, 1, 1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing for bernoulli\n",
    "x = np.array([[0, 1, 3, 4], [0, 5, 1, 4]])\n",
    "y = x\n",
    "y[y>0] = 1\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
