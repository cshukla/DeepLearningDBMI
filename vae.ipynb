{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.datasets import mnist\n",
    "\n",
    "##########################################################################\n",
    "# vae\n",
    "# \n",
    "# This function is heavily derived from - \n",
    "# \n",
    "# Inputs:\n",
    "#   - Y: matrix with gene names as counts and individual cells are rows\n",
    "#   - loss_fun: loss function for autoencoder\n",
    "#   - latent_dim: number of latent dimensions to project data into\n",
    "#   - intermediate_dim: number of dimensions in the hidden layer\n",
    "#   - batch_size: batch size used for training autoencoder\n",
    "#   - epochs: epochs for training autoencoder\n",
    "#   - nbshape: shape parameter for negative binomial loss. Defaults to 1\n",
    "# \n",
    "# Outputs:\n",
    "#   - x_encoded: Encoded transformation of input\n",
    "#   - z_mean: \n",
    "#\n",
    "##########################################################################\n",
    "\n",
    "def vae(Y, loss_fun, latent_dim, intermediate_dim, \n",
    "        batch_size, epochs, epsilon_std, nbshape=1):\n",
    "            \n",
    "    x_train = preprocess(Y, loss_fun)\n",
    "    \n",
    "    original_dim = x_train.shape[1]\n",
    "    \n",
    "    x = Input(shape=(original_dim,))\n",
    "    h = Dense(intermediate_dim, activation='relu')(x)\n",
    "    z_mean = Dense(latent_dim)(h)\n",
    "    z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    # we instantiate these layers separately so as to reuse them later\n",
    "    decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "    decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "    h_decoded = decoder_h(z)\n",
    "    x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "    # instantiate VAE model\n",
    "    model = Model(x, x_decoded_mean)\n",
    "\n",
    "    # Compute VAE loss\n",
    "    \n",
    "    model_loss = compute_model_loss(loss_fun, x, x_decoded_mean, nbshape)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    vae_loss = K.mean(model_loss + kl_loss)\n",
    "\n",
    "    model.add_loss(vae_loss)\n",
    "    model.compile(optimizer='rmsprop')\n",
    "    model.summary()\n",
    "    \n",
    "    # fit the model with the given data\n",
    "    model.fit(x_train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2)\n",
    "\n",
    "    # build a model to project inputs on the latent space\n",
    "    encoder = Model(x_train, z_mean)\n",
    "    x_encoded = encoder.predict(x_train, batch_size=batch_size)\n",
    "    \n",
    "    return x_encoded, z_mean\n",
    "\n",
    "##########################################################################\n",
    "# preprocess\n",
    "#\n",
    "# Inputs:\n",
    "#   - Y: matrix with gene names as counts and individual cells are rows\n",
    "#   - loss_fun: loss function for autoencoder\n",
    "#\n",
    "# Outputs:\n",
    "#   - x_train: preprocessed matrix of counts for training the autoencoder\n",
    "##########################################################################\n",
    "\n",
    "def preprocess(Y, loss_fun):\n",
    "    if \n",
    "    \n",
    "    return x_train\n",
    "\n",
    "##########################################################################\n",
    "# sampling\n",
    "#\n",
    "# Inputs:\n",
    "#   -\n",
    "# \n",
    "# Outputs:\n",
    "#   -\n",
    "##########################################################################\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "##########################################################################\n",
    "# compute_model_loss\n",
    "# \n",
    "# Inputs:\n",
    "#   - loss_fun: loss function for autoencoder\n",
    "#   - x: preprocessed input to the autoencoder\n",
    "#   - x_decoded_mean: mean of the x decoded by the autoencoder\n",
    "#   - nbshape: shape parameter for negative binomial loss. Defaults to 1\n",
    "# \n",
    "# Outputs:\n",
    "#   - loss: loss from the autoencoding\n",
    "##########################################################################\n",
    "\n",
    "def compute_model_loss(loss_fun, x, x_decoded_mean, nbshape):\n",
    "    if loss_fun == 'poisson':\n",
    "        loss = original_dim*poisson(x, x_decoded_mean)\n",
    "    elif loss_fun == 'negative_binomial':\n",
    "        loss = original_dim*negative_binomial(x, x_decoded_mean, nbshape)\n",
    "    elif loss_fun == 'gaussian':\n",
    "        loss = original_dim*gaussian(x, x_decoded_mean)\n",
    "    else:\n",
    "        loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n",
    "        \n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
