{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_project_vae.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "yHUvut32EKjU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "e3d06f2e-1597-4962-a0fb-097bbfcf25bc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525009749833,
          "user_tz": 240,
          "elapsed": 4414,
          "user": {
            "displayName": "Chinmay Shukla",
            "photoUrl": "//lh5.googleusercontent.com/-F7m6QcLPXpE/AAAAAAAAAAI/AAAAAAAAAC0/7vSxbPPMpi8/s50-c-k-no/photo.jpg",
            "userId": "108945151614136312923"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://www.dropbox.com/s/tdp11r39faou5ae/cortex.zip?dl=0\n",
        "!wget https://www.dropbox.com/s/o6nveqyk1k7f95z/expression_sparse.txt?dl=0"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-04-29 13:49:06--  https://www.dropbox.com/s/tdp11r39faou5ae/cortex.zip?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.9.1, 2620:100:601b:1::a27d:801\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.9.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://dl.dropboxusercontent.com/content_link/Aj5Z73aKMsoZ3Q0NCjMJBQW7H5baX5nurASBNcZBeplsc3EdE2xxUu0WCOeOWXJS/file [following]\n",
            "--2018-04-29 13:49:07--  https://dl.dropboxusercontent.com/content_link/Aj5Z73aKMsoZ3Q0NCjMJBQW7H5baX5nurASBNcZBeplsc3EdE2xxUu0WCOeOWXJS/file\n",
            "Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 162.125.9.6, 2620:100:601f:6::a27d:906\n",
            "Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|162.125.9.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2556102 (2.4M) [application/zip]\n",
            "Saving to: ‘cortex.zip?dl=0’\n",
            "\n",
            "cortex.zip?dl=0     100%[===================>]   2.44M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2018-04-29 13:49:07 (17.7 MB/s) - ‘cortex.zip?dl=0’ saved [2556102/2556102]\n",
            "\n",
            "--2018-04-29 13:49:09--  https://www.dropbox.com/s/o6nveqyk1k7f95z/expression_sparse.txt?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.9.1, 2620:100:601b:1::a27d:801\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.9.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘expression_sparse.txt?dl=0’\n",
            "\n",
            "expression_sparse.t     [  <=>               ] 186.92K   542KB/s    in 0.3s    \n",
            "\n",
            "2018-04-29 13:49:09 (542 KB/s) - ‘expression_sparse.txt?dl=0’ saved [191410]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PRd57PG7EjoR",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "from keras.layers import Input, Dense, Lambda\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras import metrics\n",
        "\n",
        "############################################################################################\n",
        "# vae: build a variational autoencoder using the input data\n",
        "# \n",
        "# This function is heavily derived from - \n",
        "# \n",
        "# Inputs:\n",
        "#   - counts_mat: matrix with gene names as counts and individual cells are rows\n",
        "#   - loss_fun: loss function for autoencoder\n",
        "#   - latent_dim: number of latent dimensions to project data into\n",
        "#   - intermediate_dim: number of dimensions in the hidden layer\n",
        "#   - batch_size: batch size used for training autoencoder\n",
        "#   - epochs: epochs for training autoencoder\n",
        "#   - nbshape: shape parameter for negative binomial loss. Defaults to 1\n",
        "# \n",
        "# Outputs:\n",
        "#   - x_encoded: Encoded transformation of input\n",
        "#   - z_mean: parameter for the mean of distribution in the latent space\n",
        "#\n",
        "############################################################################################\n",
        "\n",
        "def vae(counts_mat, loss_fun, latent_dim, intermediate_dim, \n",
        "        batch_size, epochs, epsilon_std, nbshape=1):\n",
        "            \n",
        "    x_train = preprocess(counts_mat, loss_fun)\n",
        "    original_dim = x_train.shape[1]\n",
        "    num_samples = x_train.shape[0]\n",
        "    \n",
        "    # encoder network: map inputs to latent distribution parameters\n",
        "    x = Input(shape=(original_dim,))\n",
        "    h = Dense(intermediate_dim, activation='relu')(x)\n",
        "    z_mean = Dense(latent_dim)(h)\n",
        "    z_log_var = Dense(latent_dim)(h)\n",
        "\n",
        "    \n",
        "    # use 'sampling' function to sample new similar points from the latent space\n",
        "    def sampling(args):\n",
        "      z_mean, z_log_var = args\n",
        "      epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.0, stddev=epsilon_std)\n",
        "      return z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "    \n",
        "    z = Lambda(sampling)([z_mean, z_log_var])\n",
        "    \n",
        "    # map these sampled latent points back to reconstructed inputs \n",
        "    # i.e. a sort of decoder network\n",
        "    decoder_h = Dense(intermediate_dim, activation='relu')\n",
        "    decoder_mean = Dense(original_dim, activation='sigmoid')\n",
        "    h_decoded = decoder_h(z)\n",
        "    x_decoded_mean = decoder_mean(h_decoded)\n",
        "\n",
        "\n",
        "    # build end-to-end autoencoder\n",
        "    vae_model = Model(x, x_decoded_mean)\n",
        "    \n",
        "\n",
        "    # build encoder, from inputs to latent space\n",
        "    encoder = Model(x, z_mean)\n",
        "    \n",
        "\n",
        "    # Train the model using the end-to-end model, with a custom loss function: \n",
        "    # the sum of a reconstruction term, and the KL divergence regularization term.\n",
        "    # Use different user defined loss functions to generate reconstruction loss\n",
        "    \n",
        "    if loss_fun == 'poisson':\n",
        "      reconstruction_loss = original_dim*metrics.poisson(x, x_decoded_mean)\n",
        "    elif loss_fun == 'negative_binomial':\n",
        "      reconstruction_loss = original_dim*negative_binomial_loss(x, x_decoded_mean, nbshape)\n",
        "    elif loss_fun == 'gaussian':\n",
        "      reconstruction_loss = original_dim*metrics.mean_squared_error(x, x_decoded_mean)\n",
        "    elif loss_fun == 'bernoulli':\n",
        "      reconstruction_loss = original_dim*metrics.binary_crossentropy(x, x_decoded_mean)\n",
        "    else:\n",
        "      reconstruction_loss = original_dim*metrics.binary_crossentropy(x, x_decoded_mean) # defaults to cross entropy\n",
        "        \n",
        "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "    \n",
        "    vae_loss = K.mean(reconstruction_loss + kl_loss)    \n",
        "    \n",
        "    vae_model.add_loss(vae_loss)\n",
        "    vae_model.compile(optimizer='rmsprop')\n",
        "    vae_model.summary()\n",
        "    \n",
        "    \n",
        "    # fit the model with the given data\n",
        "    vae_model.fit(x_train, shuffle=True, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n",
        "    \n",
        "\n",
        "    # project/encode inputs on the latent space\n",
        "    encoder = Model(x, z_mean)\n",
        "    x_encoded = encoder.predict(x_train, batch_size=batch_size)\n",
        "    \n",
        "\n",
        "    return x_encoded, z_mean\n",
        "\n",
        "############################################################################################\n",
        "# preprocess: preprocess counts matrix to make it suitable for particular loss function\n",
        "#\n",
        "# Inputs:\n",
        "#   - Y: matrix with gene names as counts and individual cells are rows\n",
        "#   - loss_fun: loss function for autoencoder\n",
        "#\n",
        "# Outputs:\n",
        "#   - x_train: preprocessed matrix of counts for training the autoencoder\n",
        "############################################################################################\n",
        "\n",
        "def preprocess(counts_mat, loss_fun):\n",
        "    if loss_fun == 'gaussian':\n",
        "        x_train = np.log2(1 + (counts_mat.T/np.sum(counts_mat, axis=1)).T * 1e+6) # log2(1+Y/rowSums(Y)*1e6)\n",
        "        x_train = scale(x_train, axis=1, with_mean=True, with_std=True, copy=True) # scale_center(Y)\n",
        "        \n",
        "    elif loss_fun == 'bernoulli':\n",
        "        x_train = counts_mat\n",
        "        x_train[x_train > 0] = 1 # int(Y>0)\n",
        "        \n",
        "    else:\n",
        "        x_train = counts_mat\n",
        "    \n",
        "    return x_train\n",
        "\n",
        "############################################################################################\n",
        "# sampling: sample encoded values of the input using latent space parameters\n",
        "#\n",
        "# Inputs:\n",
        "#   - args: mean and variance parameters of the latent space distribution\n",
        "# \n",
        "# Outputs:\n",
        "#   - encoded points sampled from the latent space distribution\n",
        "############################################################################################\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vpOEhqR-GYPH",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "d13ce0e3-c37a-4f01-9eb3-3f4c8544d285",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525009767666,
          "user_tz": 240,
          "elapsed": 11504,
          "user": {
            "displayName": "Chinmay Shukla",
            "photoUrl": "//lh5.googleusercontent.com/-F7m6QcLPXpE/AAAAAAAAAAI/AAAAAAAAAC0/7vSxbPPMpi8/s50-c-k-no/photo.jpg",
            "userId": "108945151614136312923"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# unzip cortex data, move it all to one directory\n",
        "!unzip cortex.zip?dl=0\n",
        "!rm cortex.zip?dl=0\n",
        "!mkdir cortex\n",
        "!mv cell_info.csv cortex/\n",
        "!mv cortex_expression.txt cortex/\n",
        "!mv gene_info.csv cortex/\n",
        "\n",
        "# move CBMC data to one directory\n",
        "!mkdir CBMC_sparse\n",
        "!mv expression_sparse.txt?dl=0 expression_sparse.txt\n",
        "!mv expression_sparse.txt CBMC_sparse/\n",
        "\n",
        "!ls"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  cortex.zip?dl=0\r\n",
            "  inflating: cell_info.csv           \n",
            "  inflating: cortex_expression.txt   \n",
            "  inflating: gene_info.csv           \n",
            "mkdir: cannot create directory ‘cortex’: File exists\n",
            "mkdir: cannot create directory ‘CBMC_sparse’: File exists\n",
            "CBMC_sparse  cortex  datalab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hxouQ_OUeWh7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 3913
        },
        "outputId": "27eb9af4-266e-4b69-92be-192f2e4eaf65",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525009878095,
          "user_tz": 240,
          "elapsed": 41322,
          "user": {
            "displayName": "Chinmay Shukla",
            "photoUrl": "//lh5.googleusercontent.com/-F7m6QcLPXpE/AAAAAAAAAAI/AAAAAAAAAC0/7vSxbPPMpi8/s50-c-k-no/photo.jpg",
            "userId": "108945151614136312923"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "cortex_counts = np.loadtxt('cortex/cortex_expression.txt')\n",
        "cortex_counts = cortex_counts.T.astype('float32')\n",
        "encoded_cortex_counts, cortex_counts_z_mean = vae(cortex_counts, loss_fun = 'gaussian', latent_dim = 50, intermediate_dim = 500, batch_size = 100, epochs = 100, epsilon_std = 1, nbshape=1)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\"Numerical issues were encountered \"\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
            "  warnings.warn(\"Numerical issues were encountered \"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: Output \"dense_78\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"dense_78\" during training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_16 (InputLayer)           (None, 2000)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_74 (Dense)                (None, 500)          1000500     input_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_75 (Dense)                (None, 50)           25050       dense_74[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_76 (Dense)                (None, 50)           25050       dense_74[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_15 (Lambda)              (None, 50)           0           dense_75[0][0]                   \n",
            "                                                                 dense_76[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_77 (Dense)                (None, 500)          25500       lambda_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_78 (Dense)                (None, 2000)         1002000     dense_77[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 2,078,100\n",
            "Trainable params: 2,078,100\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 2704 samples, validate on 301 samples\n",
            "Epoch 1/100\n",
            "2704/2704 [==============================] - 1s 387us/step - loss: 1641.5449 - val_loss: 1788.0233\n",
            "Epoch 2/100\n",
            "2704/2704 [==============================] - 0s 140us/step - loss: 1415.4111 - val_loss: 1715.0324\n",
            "Epoch 3/100\n",
            "2704/2704 [==============================] - 0s 134us/step - loss: 1380.9706 - val_loss: 1686.2574\n",
            "Epoch 4/100\n",
            "2704/2704 [==============================] - 0s 136us/step - loss: 1363.5302 - val_loss: 1673.7929\n",
            "Epoch 5/100\n",
            "2704/2704 [==============================] - 0s 134us/step - loss: 1351.5772 - val_loss: 1665.0709\n",
            "Epoch 6/100\n",
            " 500/2704 [====>.........................] - ETA: 0s - loss: 1354.0429"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2704/2704 [==============================] - 0s 134us/step - loss: 1344.7618 - val_loss: 1678.7485\n",
            "Epoch 7/100\n",
            "2704/2704 [==============================] - 0s 135us/step - loss: 1339.7140 - val_loss: 1673.4911\n",
            "Epoch 8/100\n",
            "2704/2704 [==============================] - 0s 136us/step - loss: 1336.2018 - val_loss: 1658.6051\n",
            "Epoch 9/100\n",
            "2704/2704 [==============================] - 0s 134us/step - loss: 1332.6018 - val_loss: 1671.1622\n",
            "Epoch 10/100\n",
            "2704/2704 [==============================] - 0s 134us/step - loss: 1330.4226 - val_loss: 1657.6675\n",
            "Epoch 11/100\n",
            "2704/2704 [==============================] - 0s 128us/step - loss: 1327.7597 - val_loss: 1660.1555\n",
            "Epoch 12/100\n",
            "2704/2704 [==============================] - 0s 130us/step - loss: 1326.6340 - val_loss: 1655.4496\n",
            "Epoch 13/100\n",
            "2704/2704 [==============================] - 0s 132us/step - loss: 1324.7642 - val_loss: 1652.5421\n",
            "Epoch 14/100\n",
            "2704/2704 [==============================] - 0s 136us/step - loss: 1324.0218 - val_loss: 1662.0865\n",
            "Epoch 15/100\n",
            "2704/2704 [==============================] - 0s 134us/step - loss: 1322.5881 - val_loss: 1653.7847\n",
            "Epoch 16/100\n",
            "2704/2704 [==============================] - 0s 133us/step - loss: 1321.4627 - val_loss: 1653.5054\n",
            "Epoch 17/100\n",
            "2704/2704 [==============================] - 0s 130us/step - loss: 1319.6275 - val_loss: 1655.4400\n",
            "Epoch 18/100\n",
            "2704/2704 [==============================] - 0s 132us/step - loss: 1319.7282 - val_loss: 1645.4591\n",
            "Epoch 19/100\n",
            "2704/2704 [==============================] - 0s 132us/step - loss: 1318.2681 - val_loss: 1651.3032\n",
            "Epoch 20/100\n",
            "2704/2704 [==============================] - 0s 134us/step - loss: 1316.9659 - val_loss: 1642.2339\n",
            "Epoch 21/100\n",
            "1000/2704 [==========>...................] - ETA: 0s - loss: 1317.0545"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2704/2704 [==============================] - 0s 134us/step - loss: 1316.3547 - val_loss: 1648.8987\n",
            "Epoch 22/100\n",
            "2704/2704 [==============================] - 0s 131us/step - loss: 1315.3806 - val_loss: 1650.2462\n",
            "Epoch 23/100\n",
            "2704/2704 [==============================] - 0s 131us/step - loss: 1314.3546 - val_loss: 1669.4638\n",
            "Epoch 24/100\n",
            "2704/2704 [==============================] - 0s 131us/step - loss: 1313.9227 - val_loss: 1636.3868\n",
            "Epoch 25/100\n",
            "2704/2704 [==============================] - 0s 132us/step - loss: 1312.5825 - val_loss: 1635.0308\n",
            "Epoch 26/100\n",
            "2704/2704 [==============================] - 0s 132us/step - loss: 1312.1751 - val_loss: 1630.9414\n",
            "Epoch 27/100\n",
            "2704/2704 [==============================] - 0s 132us/step - loss: 1310.8565 - val_loss: 1627.6214\n",
            "Epoch 28/100\n",
            "2704/2704 [==============================] - 0s 133us/step - loss: 1310.1658 - val_loss: 1627.7738\n",
            "Epoch 29/100\n",
            "2704/2704 [==============================] - 0s 138us/step - loss: 1309.6444 - val_loss: 1636.1333\n",
            "Epoch 30/100\n",
            "2704/2704 [==============================] - 0s 135us/step - loss: 1309.0644 - val_loss: 1635.6225\n",
            "Epoch 31/100\n",
            "2704/2704 [==============================] - 0s 132us/step - loss: 1308.6943 - val_loss: 1625.1550\n",
            "Epoch 32/100\n",
            "2704/2704 [==============================] - 0s 129us/step - loss: 1307.5829 - val_loss: 1625.4583\n",
            "Epoch 33/100\n",
            "2704/2704 [==============================] - 0s 127us/step - loss: 1307.1774 - val_loss: 1619.0191\n",
            "Epoch 34/100\n",
            "2704/2704 [==============================] - 0s 138us/step - loss: 1306.9418 - val_loss: 1625.1652\n",
            "Epoch 35/100\n",
            "2704/2704 [==============================] - 0s 130us/step - loss: 1305.8671 - val_loss: 1615.9595\n",
            "Epoch 36/100\n",
            " 100/2704 [>.............................] - ETA: 0s - loss: 1316.7052"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2704/2704 [==============================] - 0s 132us/step - loss: 1304.9166 - val_loss: 1629.5682\n",
            "Epoch 37/100\n",
            "2704/2704 [==============================] - 0s 133us/step - loss: 1305.0530 - val_loss: 1629.0922\n",
            "Epoch 38/100\n",
            "2704/2704 [==============================] - 0s 128us/step - loss: 1304.1069 - val_loss: 1606.6340\n",
            "Epoch 39/100\n",
            "2704/2704 [==============================] - 0s 129us/step - loss: 1303.5082 - val_loss: 1602.9646\n",
            "Epoch 40/100\n",
            "2704/2704 [==============================] - 0s 132us/step - loss: 1303.0491 - val_loss: 1617.6061\n",
            "Epoch 41/100\n",
            "2704/2704 [==============================] - 0s 127us/step - loss: 1302.3341 - val_loss: 1602.1371\n",
            "Epoch 42/100\n",
            "2704/2704 [==============================] - 0s 126us/step - loss: 1301.7397 - val_loss: 1605.1137\n",
            "Epoch 43/100\n",
            "2704/2704 [==============================] - 0s 131us/step - loss: 1301.3303 - val_loss: 1601.0110\n",
            "Epoch 44/100\n",
            "2704/2704 [==============================] - 0s 135us/step - loss: 1300.9107 - val_loss: 1607.5635\n",
            "Epoch 45/100\n",
            "2704/2704 [==============================] - 0s 136us/step - loss: 1300.1814 - val_loss: 1601.7823\n",
            "Epoch 46/100\n",
            "2704/2704 [==============================] - 0s 137us/step - loss: 1299.7037 - val_loss: 1600.3189\n",
            "Epoch 47/100\n",
            "2704/2704 [==============================] - 0s 133us/step - loss: 1299.1053 - val_loss: 1602.9731\n",
            "Epoch 48/100\n",
            "2704/2704 [==============================] - 0s 136us/step - loss: 1298.6172 - val_loss: 1603.7272\n",
            "Epoch 49/100\n",
            "2704/2704 [==============================] - 0s 127us/step - loss: 1298.4089 - val_loss: 1595.6028\n",
            "Epoch 50/100\n",
            "2704/2704 [==============================] - 0s 130us/step - loss: 1298.0429 - val_loss: 1590.9152\n",
            "Epoch 51/100\n",
            " 500/2704 [====>.........................] - ETA: 0s - loss: 1301.5928"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2704/2704 [==============================] - 0s 132us/step - loss: 1297.0471 - val_loss: 1596.4159\n",
            "Epoch 52/100\n",
            "2704/2704 [==============================] - 0s 139us/step - loss: 1297.2050 - val_loss: 1586.8859\n",
            "Epoch 53/100\n",
            "2704/2704 [==============================] - 0s 129us/step - loss: 1296.5483 - val_loss: 1587.8474\n",
            "Epoch 54/100\n",
            "2704/2704 [==============================] - 0s 128us/step - loss: 1296.6898 - val_loss: 1589.8949\n",
            "Epoch 55/100\n",
            "2704/2704 [==============================] - 0s 129us/step - loss: 1295.6949 - val_loss: 1587.3476\n",
            "Epoch 56/100\n",
            "2704/2704 [==============================] - 0s 132us/step - loss: 1295.4613 - val_loss: 1584.5341\n",
            "Epoch 57/100\n",
            "2704/2704 [==============================] - 0s 134us/step - loss: 1294.7039 - val_loss: 1591.3459\n",
            "Epoch 58/100\n",
            "2704/2704 [==============================] - 0s 137us/step - loss: 1294.4726 - val_loss: 1586.7800\n",
            "Epoch 59/100\n",
            "2704/2704 [==============================] - 0s 135us/step - loss: 1293.9055 - val_loss: 1584.6343\n",
            "Epoch 60/100\n",
            "2704/2704 [==============================] - 0s 141us/step - loss: 1293.9046 - val_loss: 1582.5015\n",
            "Epoch 61/100\n",
            "2704/2704 [==============================] - 0s 154us/step - loss: 1293.6184 - val_loss: 1583.0143\n",
            "Epoch 62/100\n",
            "2704/2704 [==============================] - 0s 132us/step - loss: 1293.0679 - val_loss: 1583.0746\n",
            "Epoch 63/100\n",
            "2704/2704 [==============================] - 0s 134us/step - loss: 1292.7419 - val_loss: 1588.3574\n",
            "Epoch 64/100\n",
            "2704/2704 [==============================] - 0s 134us/step - loss: 1292.1835 - val_loss: 1582.0987\n",
            "Epoch 65/100\n",
            "2704/2704 [==============================] - 0s 132us/step - loss: 1291.9118 - val_loss: 1583.8997\n",
            "Epoch 66/100\n",
            " 600/2704 [=====>........................] - ETA: 0s - loss: 1297.5171"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2704/2704 [==============================] - 0s 145us/step - loss: 1291.6819 - val_loss: 1577.4060\n",
            "Epoch 67/100\n",
            "2704/2704 [==============================] - 0s 128us/step - loss: 1291.1956 - val_loss: 1582.5473\n",
            "Epoch 68/100\n",
            "2704/2704 [==============================] - 0s 130us/step - loss: 1290.5235 - val_loss: 1581.0298\n",
            "Epoch 69/100\n",
            "2704/2704 [==============================] - 0s 129us/step - loss: 1290.5948 - val_loss: 1591.1243\n",
            "Epoch 70/100\n",
            "2704/2704 [==============================] - 0s 129us/step - loss: 1290.1640 - val_loss: 1596.3091\n",
            "Epoch 71/100\n",
            "2704/2704 [==============================] - 0s 129us/step - loss: 1289.7990 - val_loss: 1588.3214\n",
            "Epoch 72/100\n",
            "2704/2704 [==============================] - 0s 131us/step - loss: 1289.5046 - val_loss: 1576.4429\n",
            "Epoch 73/100\n",
            "2704/2704 [==============================] - 0s 148us/step - loss: 1289.3289 - val_loss: 1583.8658\n",
            "Epoch 74/100\n",
            "2704/2704 [==============================] - 0s 136us/step - loss: 1288.9663 - val_loss: 1580.7954\n",
            "Epoch 75/100\n",
            "2704/2704 [==============================] - 0s 131us/step - loss: 1288.4834 - val_loss: 1574.8950\n",
            "Epoch 76/100\n",
            "2704/2704 [==============================] - 0s 130us/step - loss: 1288.5769 - val_loss: 1584.7789\n",
            "Epoch 77/100\n",
            "2704/2704 [==============================] - 0s 129us/step - loss: 1287.7215 - val_loss: 1582.9509\n",
            "Epoch 78/100\n",
            "2704/2704 [==============================] - 0s 131us/step - loss: 1287.6251 - val_loss: 1580.9292\n",
            "Epoch 79/100\n",
            "2704/2704 [==============================] - 0s 129us/step - loss: 1287.2702 - val_loss: 1571.9992\n",
            "Epoch 80/100\n",
            "2704/2704 [==============================] - 0s 137us/step - loss: 1286.8929 - val_loss: 1579.8040\n",
            "Epoch 81/100\n",
            " 500/2704 [====>.........................] - ETA: 0s - loss: 1290.5402"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2704/2704 [==============================] - 0s 132us/step - loss: 1286.8162 - val_loss: 1580.4022\n",
            "Epoch 82/100\n",
            "2704/2704 [==============================] - 0s 126us/step - loss: 1286.4660 - val_loss: 1571.6062\n",
            "Epoch 83/100\n",
            "2704/2704 [==============================] - 0s 130us/step - loss: 1285.9305 - val_loss: 1571.7184\n",
            "Epoch 84/100\n",
            "2704/2704 [==============================] - 0s 129us/step - loss: 1286.1777 - val_loss: 1569.3128\n",
            "Epoch 85/100\n",
            "2704/2704 [==============================] - 0s 126us/step - loss: 1285.1772 - val_loss: 1575.4701\n",
            "Epoch 86/100\n",
            "2704/2704 [==============================] - 0s 129us/step - loss: 1285.1383 - val_loss: 1572.2560\n",
            "Epoch 87/100\n",
            "2704/2704 [==============================] - 0s 132us/step - loss: 1284.8775 - val_loss: 1577.9786\n",
            "Epoch 88/100\n",
            "2704/2704 [==============================] - 0s 132us/step - loss: 1284.4710 - val_loss: 1587.1829\n",
            "Epoch 89/100\n",
            "2704/2704 [==============================] - 0s 132us/step - loss: 1284.3145 - val_loss: 1591.1600\n",
            "Epoch 90/100\n",
            "2704/2704 [==============================] - 0s 133us/step - loss: 1284.0326 - val_loss: 1572.0633\n",
            "Epoch 91/100\n",
            "2704/2704 [==============================] - 0s 136us/step - loss: 1283.7779 - val_loss: 1580.6609\n",
            "Epoch 92/100\n",
            "2704/2704 [==============================] - 0s 133us/step - loss: 1283.7005 - val_loss: 1574.5788\n",
            "Epoch 93/100\n",
            "2704/2704 [==============================] - 0s 131us/step - loss: 1283.3008 - val_loss: 1575.7668\n",
            "Epoch 94/100\n",
            "2704/2704 [==============================] - 0s 132us/step - loss: 1283.0445 - val_loss: 1582.7820\n",
            "Epoch 95/100\n",
            "2704/2704 [==============================] - 0s 134us/step - loss: 1282.8058 - val_loss: 1575.5916\n",
            "Epoch 96/100\n",
            "1000/2704 [==========>...................] - ETA: 0s - loss: 1279.7876"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2704/2704 [==============================] - 0s 129us/step - loss: 1282.2582 - val_loss: 1574.8333\n",
            "Epoch 97/100\n",
            "2704/2704 [==============================] - 0s 129us/step - loss: 1282.1879 - val_loss: 1578.9421\n",
            "Epoch 98/100\n",
            "2704/2704 [==============================] - 0s 135us/step - loss: 1281.5851 - val_loss: 1570.7933\n",
            "Epoch 99/100\n",
            "2704/2704 [==============================] - 0s 136us/step - loss: 1281.9055 - val_loss: 1576.5374\n",
            "Epoch 100/100\n",
            "2704/2704 [==============================] - 0s 133us/step - loss: 1281.6269 - val_loss: 1575.4139\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}